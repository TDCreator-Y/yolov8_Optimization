# YOLOv8ä¼˜åŒ–ç¤ºä¾‹é¡¹ç›® - å·¥å…·æ¨¡å—
# ============================

"""
YOLOv8ä¼˜åŒ–ç¤ºä¾‹é¡¹ç›®çš„æ ¸å¿ƒå·¥å…·æ¨¡å—

æœ¬æ¨¡å—æä¾›äº†ä¸€å¥—å®Œæ•´çš„å·¥å…·å‡½æ•°é›†åˆï¼Œç”¨äºæ”¯æŒYOLOv8æ¨¡å‹çš„å¼€å‘ã€æµ‹è¯•å’Œä¼˜åŒ–è¿‡ç¨‹ã€‚
æ¶µç›–äº†ä»è®¾å¤‡ç®¡ç†åˆ°æ€§èƒ½è¯„ä¼°çš„å„ä¸ªæ–¹é¢ï¼Œä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹å¼€å‘æä¾›å…¨æ–¹ä½çš„æ”¯æŒã€‚

ä¸»è¦åŠŸèƒ½æ¨¡å—ï¼š
====================
1. è®¾å¤‡ç®¡ç† (Device Management)
   - setup_device(): æ™ºèƒ½è®¾å¤‡æ£€æµ‹å’Œé…ç½®
   - è‡ªåŠ¨æ£€æµ‹GPUå¯ç”¨æ€§å¹¶æä¾›å¤‡ç”¨æ–¹æ¡ˆ

2. æ¨¡å‹åˆ†æ (Model Analysis)
   - count_parameters(): è¯¦ç»†çš„å‚æ•°ç»Ÿè®¡
   - åŒºåˆ†å¯è®­ç»ƒå’Œä¸å¯è®­ç»ƒå‚æ•°
   - æä¾›æ˜“è¯»çš„ç»Ÿè®¡æŠ¥å‘Š

3. æ¨¡å‹éªŒè¯ (Model Validation)
   - validate_model(): å®Œæ•´æ€§éªŒè¯
   - å‰å‘ä¼ æ’­æµ‹è¯•å’Œè¾“å‡ºæ£€æŸ¥
   - è®¾å¤‡å…¼å®¹æ€§éªŒè¯

4. æ€§èƒ½åŸºå‡†æµ‹è¯• (Performance Benchmarking)
   - benchmark_inference(): æ¨ç†é€Ÿåº¦æµ‹è¯•
   - GPU/CPUæ€§èƒ½å¯¹æ¯”
   - è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡æŠ¥å‘Š

æŠ€æœ¯ç‰¹ç‚¹ï¼š
==========
- å…¼å®¹æ€§ï¼šæ”¯æŒPyTorchç”Ÿæ€ç³»ç»Ÿ
- æ˜“ç”¨æ€§ï¼šç»Ÿä¸€çš„APIè®¾è®¡å’Œè¯¦ç»†æ–‡æ¡£
- å¯é æ€§ï¼šå®Œå–„çš„é”™è¯¯å¤„ç†å’Œå¼‚å¸¸æ•è·
- æ‰©å±•æ€§ï¼šæ¨¡å—åŒ–è®¾è®¡ä¾¿äºåŠŸèƒ½æ‰©å±•

ä½¿ç”¨åœºæ™¯ï¼š
==========
- æ¨¡å‹å¼€å‘ï¼šå¿«é€ŸéªŒè¯æ¨¡å‹æ¶æ„æ­£ç¡®æ€§
- æ€§èƒ½ä¼˜åŒ–ï¼šåŸºå‡†æµ‹è¯•å’Œæ€§èƒ½å¯¹æ¯”
- éƒ¨ç½²å‡†å¤‡ï¼šè®¾å¤‡å…¼å®¹æ€§æ£€æŸ¥å’Œæ€§èƒ½è¯„ä¼°
- è°ƒè¯•è¯Šæ–­ï¼šè¯¦ç»†çš„å‚æ•°å’Œæ€§èƒ½ç»Ÿè®¡

ä¾èµ–å…³ç³»ï¼š
==========
- torch: æ ¸å¿ƒæ·±åº¦å­¦ä¹ æ¡†æ¶
- torch.nn: ç¥ç»ç½‘ç»œæ¨¡å—
- ultralytics.utils.torch_utils: YOLOv8å·¥å…·å‡½æ•°
"""

import torch
import torch.nn as nn
from ultralytics.utils.torch_utils import smart_inference_mode

def setup_device():
    """
    æ™ºèƒ½è®¾å¤‡æ£€æµ‹å’Œé…ç½®å‡½æ•°

    è‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿä¸­çš„å¯ç”¨è®¡ç®—è®¾å¤‡ï¼Œå¹¶æŒ‰ç…§æ€§èƒ½ä¼˜å…ˆçº§é€‰æ‹©æœ€ä¼˜è®¾å¤‡ã€‚
    è¯¥å‡½æ•°æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹éƒ¨ç½²çš„åŸºç¡€ç»„ä»¶ï¼Œç¡®ä¿åœ¨ä¸åŒç¡¬ä»¶ç¯å¢ƒä¸‹éƒ½èƒ½æ­£å¸¸å·¥ä½œã€‚

    è®¾å¤‡é€‰æ‹©ç­–ç•¥ï¼š
    ================
    ä¼˜å…ˆçº§æ’åºï¼š
    1. CUDA GPU (æœ€é«˜ä¼˜å…ˆçº§) - å¹¶è¡Œè®¡ç®—èƒ½åŠ›å¼ºï¼Œé€‚åˆå¤§è§„æ¨¡æ¨¡å‹
    2. CPU (å¤‡ç”¨é€‰é¡¹) - é€šç”¨å¤„ç†å™¨ï¼Œå…¼å®¹æ€§æœ€å¥½

    GPUæ£€æµ‹é€»è¾‘ï¼š
    =============
    - æ£€æŸ¥torch.cuda.is_available()çš„è¿”å›å€¼
    - è·å–GPUè®¾å¤‡å±æ€§ï¼šåç§°ã€CUDAç‰ˆæœ¬ã€æ˜¾å­˜å®¹é‡
    - éªŒè¯GPUæ˜¯å¦å¯æ­£å¸¸è®¿é—®å’Œä½¿ç”¨

    è¿”å›å€¼ï¼š
    =======
    torch.device: PyTorchè®¾å¤‡å¯¹è±¡
        - cuda: CUDA GPUè®¾å¤‡ï¼ˆæ¨èï¼‰
        - cpu: CPUè®¾å¤‡ï¼ˆå¤‡ç”¨ï¼‰

    è¾“å‡ºä¿¡æ¯ï¼š
    =========
    GPUæ¨¡å¼ï¼š
        âœ… ä½¿ç”¨GPU: [GPUå‹å·åç§°]
           CUDAç‰ˆæœ¬: [CUDAç‰ˆæœ¬å·]
           GPUæ˜¾å­˜: [æ˜¾å­˜å¤§å°] GB

    CPUæ¨¡å¼ï¼š
        âš ï¸ ä½¿ç”¨CPU (GPUä¸å¯ç”¨)

    æŠ€æœ¯ç»†èŠ‚ï¼š
    =========
    - GPUä¿¡æ¯è·å–ï¼šä½¿ç”¨torch.cudaæ¨¡å—çš„è®¾å¤‡å±æ€§API
    - æ˜¾å­˜è®¡ç®—ï¼šå°†å­—èŠ‚è½¬æ¢ä¸ºGBï¼Œä¿ç•™ä¸€ä½å°æ•°
    - CUDAç‰ˆæœ¬ï¼šé€šè¿‡torch.version.cudaè·å–ç¼–è¯‘æ—¶ç‰ˆæœ¬
    - è®¾å¤‡å¯¹è±¡ï¼štorch.deviceå°è£…ï¼Œä¾¿äºæ¨¡å‹éƒ¨ç½²

    å¼‚å¸¸å¤„ç†ï¼š
    =========
    - GPUä¸å¯ç”¨æ—¶è‡ªåŠ¨é™çº§åˆ°CPU
    - ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç¡®ä¿ç¨‹åºæ­£å¸¸è¿è¡Œ
    - æä¾›æ¸…æ™°çš„çŠ¶æ€æç¤ºä¿¡æ¯

    ä½¿ç”¨å»ºè®®ï¼š
    =========
    - åœ¨æ¨¡å‹åˆå§‹åŒ–é˜¶æ®µè°ƒç”¨
    - ä¿å­˜è¿”å›å€¼ä¾›åç»­ä½¿ç”¨
    - GPUç¯å¢ƒä¸‹å»ºè®®æ£€æŸ¥æ˜¾å­˜æ˜¯å¦å……è¶³
    - ç”Ÿäº§ç¯å¢ƒå»ºè®®æ·»åŠ æ›´è¯¦ç»†çš„è®¾å¤‡æ£€æµ‹

    ç¤ºä¾‹ä»£ç ï¼š
    =========
    >>> # åŸºæœ¬ä½¿ç”¨
    >>> device = setup_device()
    >>> model = model.to(device)

    >>> # ç»“åˆæ¨¡å‹éƒ¨ç½²
    >>> device = setup_device()
    >>> model = MyModel().to(device)
    >>> optimizer = torch.optim.Adam(model.parameters())

    >>> # æ¡ä»¶åˆ¤æ–­ä½¿ç”¨
    >>> device = setup_device()
    >>> if device.type == 'cuda':
    ...     print("å¯ç”¨GPUåŠ é€Ÿ")
    ... else:
    ...     print("ä½¿ç”¨CPUè®¡ç®—")

    æ€§èƒ½å½±å“ï¼š
    =========
    - GPUæ¨¡å¼ï¼šæ˜¾è‘—æå‡è®¡ç®—é€Ÿåº¦ï¼Œé€‚åˆè®­ç»ƒå’Œæ¨ç†
    - CPUæ¨¡å¼ï¼šè®¡ç®—é€Ÿåº¦è¾ƒæ…¢ï¼Œä½†å†…å­˜å ç”¨è¾ƒå°‘
    - æ£€æµ‹å¼€é”€ï¼šå‡½æ•°è°ƒç”¨è€—æ—¶å¾ˆå°ï¼Œå¯å¿½ç•¥ä¸è®¡

    å…¼å®¹æ€§ï¼š
    ========
    - PyTorch 1.0+: å®Œå…¨å…¼å®¹
    - CUDA 9.0+: æ”¯æŒç°ä»£GPUæ¶æ„
    - CPU: æ”¯æŒæ‰€æœ‰x86/x64æ¶æ„
    """
    # æ ¸å¿ƒè®¾å¤‡æ£€æµ‹é€»è¾‘
    if torch.cuda.is_available():
        # GPUåˆ†æ”¯ï¼šä½¿ç”¨CUDAè¿›è¡Œç¡¬ä»¶åŠ é€Ÿ
        device = torch.device('cuda')

        # è·å–è¯¦ç»†çš„GPUç¡¬ä»¶ä¿¡æ¯
        gpu_name = torch.cuda.get_device_name(0)  # GPUå‹å·åç§°
        cuda_version = torch.version.cuda          # CUDAç¼–è¯‘ç‰ˆæœ¬
        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # æ˜¾å­˜å®¹é‡(GB)

        # è¾“å‡ºè¯¦ç»†çš„GPUé…ç½®ä¿¡æ¯
        print("âœ… ä½¿ç”¨GPUè¿›è¡Œç¡¬ä»¶åŠ é€Ÿè®¡ç®—")
        print(f"   GPUå‹å·: {gpu_name}")
        print(f"   CUDAç‰ˆæœ¬: {cuda_version}")
        print(f"   GPUæ˜¾å­˜: {gpu_memory_gb:.1f} GB")

        # æä¾›ä½¿ç”¨å»ºè®®
        if gpu_memory_gb < 4.0:
            print("   ğŸ’¡ æç¤º: GPUæ˜¾å­˜è¾ƒå°ï¼Œå»ºè®®ä½¿ç”¨è¾ƒå°çš„batch_size")
        else:
            print("   ğŸ’¡ æç¤º: GPUé…ç½®è‰¯å¥½ï¼Œé€‚åˆå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒ")

    else:
        # CPUåˆ†æ”¯ï¼šGPUä¸å¯ç”¨æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆ
        device = torch.device('cpu')

        # è¾“å‡ºCPUä½¿ç”¨æç¤º
        print("âš ï¸  GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè¿›è¡Œè®¡ç®—")
        print("   ğŸ’¡ æç¤º: è®¡ç®—é€Ÿåº¦å¯èƒ½è¾ƒæ…¢ï¼Œé€‚åˆå°è§„æ¨¡æ¨¡å‹æˆ–æµ‹è¯•")

        # æ£€æµ‹CPUæ ¸å¿ƒæ•°ï¼ˆå¯é€‰ä¿¡æ¯ï¼‰
        import multiprocessing
        cpu_count = multiprocessing.cpu_count()
        print(f"   CPUæ ¸å¿ƒæ•°: {cpu_count}")

    # è¿”å›é…ç½®å¥½çš„è®¾å¤‡å¯¹è±¡
    return device

def count_parameters(model):
    """
    æ·±åº¦å­¦ä¹ æ¨¡å‹å‚æ•°ç»Ÿè®¡åˆ†æå‡½æ•°

    å¯¹PyTorchæ¨¡å‹è¿›è¡Œå…¨é¢çš„å‚æ•°ç»Ÿè®¡åˆ†æï¼Œæä¾›æ¨¡å‹å¤æ‚åº¦è¯„ä¼°çš„é‡è¦æŒ‡æ ‡ã€‚
    è¯¥å‡½æ•°æ˜¯æ¨¡å‹åˆ†æçš„åŸºç¡€å·¥å…·ï¼Œå¸®åŠ©å¼€å‘è€…äº†è§£æ¨¡å‹è§„æ¨¡å’Œè®­ç»ƒéœ€æ±‚ã€‚

    ç»Ÿè®¡å†…å®¹ï¼š
    =========
    1. æ€»å‚æ•°é‡ (Total Parameters)
       - æ¨¡å‹ä¸­æ‰€æœ‰å‚æ•°çš„æ€»æ•°
       - åŒ…æ‹¬æƒé‡çŸ©é˜µå’Œåç½®é¡¹
       - åæ˜ æ¨¡å‹çš„æ•´ä½“å¤æ‚åº¦

    2. å¯è®­ç»ƒå‚æ•°é‡ (Trainable Parameters)
       - requires_grad=Trueçš„å‚æ•°
       - å‚ä¸æ¢¯åº¦æ›´æ–°å’Œåå‘ä¼ æ’­
       - å†³å®šè®­ç»ƒæ—¶å†…å­˜å’Œè®¡ç®—å¼€é”€

    3. å†»ç»“å‚æ•°é‡ (Frozen Parameters)
       - requires_grad=Falseçš„å‚æ•°
       - ä¸å‚ä¸è®­ç»ƒï¼Œé€šå¸¸æ¥è‡ªé¢„è®­ç»ƒæ¨¡å‹
       - å½±å“æ¨¡å‹çš„æ¨ç†é€Ÿåº¦

    å‚æ•°åˆ†æï¼š
    =========
    model (torch.nn.Module): PyTorchç¥ç»ç½‘ç»œæ¨¡å‹
        - æ”¯æŒæ‰€æœ‰PyTorchæ¨¡å‹ç±»å‹
        - åŒ…æ‹¬è‡ªå®šä¹‰æ¨¡å‹å’Œé¢„è®­ç»ƒæ¨¡å‹
        - é€’å½’ç»Ÿè®¡æ‰€æœ‰å­æ¨¡å—å‚æ•°

    è¿”å›å€¼ï¼š
    =======
    tuple: (æ€»å‚æ•°é‡, å¯è®­ç»ƒå‚æ•°é‡)
        - total_params (int): æ¨¡å‹æ€»å‚æ•°æ•°é‡
        - trainable_params (int): å¯è®­ç»ƒå‚æ•°æ•°é‡

    è®¡ç®—æ–¹æ³•ï¼š
    =========
    - ä½¿ç”¨p.numel()è·å–æ¯ä¸ªå‚æ•°å¼ é‡çš„å…ƒç´ æ•°é‡
    - é€šè¿‡p.requires_gradåˆ¤æ–­å‚æ•°æ˜¯å¦å¯è®­ç»ƒ
    - é€’å½’éå†model.parameters()è·å–æ‰€æœ‰å‚æ•°

    æ˜¾ç¤ºæ ¼å¼ï¼š
    =========
    ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:
       æ€»è®¡: 3,257,984 (3.26M)
       å¯è®­ç»ƒ: 3,257,984 (3.26M)
       ä¸å¯è®­ç»ƒ: 0

    æ ¼å¼è¯´æ˜ï¼š
    - åŸå§‹æ•°å­—ï¼šä½¿ç”¨åƒåˆ†ä½åˆ†éš”ç¬¦
    - ç™¾ä¸‡å•ä½ï¼šè½¬æ¢ä¸ºMå•ä½ï¼Œä¿ç•™2ä½å°æ•°
    - é¢œè‰²ç¼–ç ï¼šä¸åŒå‚æ•°ç±»å‹ç”¨ä¸åŒé¢œè‰²æ˜¾ç¤º

    åº”ç”¨åœºæ™¯ï¼š
    ==========
    1. æ¨¡å‹å¤æ‚åº¦è¯„ä¼°
       - åˆ¤æ–­æ¨¡å‹æ˜¯å¦è¿‡äºåºå¤§
       - è¯„ä¼°è®­ç»ƒå’Œéƒ¨ç½²æˆæœ¬

    2. è¿ç§»å­¦ä¹ åˆ†æ
       - æŸ¥çœ‹å†»ç»“å‚æ•°çš„æ¯”ä¾‹
       - äº†è§£é¢„è®­ç»ƒæ¨¡å‹çš„ä½¿ç”¨æƒ…å†µ

    3. æ¨¡å‹æ¯”è¾ƒ
       - å¯¹æ¯”ä¸åŒæ¶æ„çš„å‚æ•°æ•ˆç‡
       - é€‰æ‹©åˆé€‚çš„æ¨¡å‹å¤§å°

    4. ç¡¬ä»¶èµ„æºè§„åˆ’
       - ä¼°ç®—GPUå†…å­˜éœ€æ±‚
       - è§„åˆ’è®­ç»ƒé›†ç¾¤è§„æ¨¡

    æ€§èƒ½å½±å“ï¼š
    =========
    - è®¡ç®—å¼€é”€ï¼šéå†æ‰€æœ‰å‚æ•°ï¼Œè€—æ—¶å¾ˆå°
    - å†…å­˜ä½¿ç”¨ï¼šä¸é¢å¤–åˆ†é…å†…å­˜
    - è°ƒç”¨é¢‘ç‡ï¼šå¯åœ¨è®­ç»ƒå‰åè°ƒç”¨å¤šæ¬¡

    æŠ€æœ¯ç»†èŠ‚ï¼š
    =========
    - å‚æ•°è®¡æ•°ï¼šä½¿ç”¨tensor.numel()æ–¹æ³•
    - æ¢¯åº¦æ£€æŸ¥ï¼šé€šè¿‡requires_gradå±æ€§åˆ¤æ–­
    - æ ¼å¼åŒ–è¾“å‡ºï¼šä½¿ç”¨f-stringå’Œæ ¼å¼è¯´æ˜ç¬¦
    - å•ä½è½¬æ¢ï¼šè‡ªåŠ¨è½¬æ¢ä¸ºæ˜“è¯»çš„Må•ä½

    æ³¨æ„äº‹é¡¹ï¼š
    =========
    - åªç»Ÿè®¡torch.nn.Parameterç±»å‹çš„å‚æ•°
    - ä¸åŒ…æ‹¬ç¼“å†²åŒº(buffer)å‚æ•°
    - BatchNormçš„running_meanç­‰ä¸è®¡å…¥å‚æ•°é‡
    - å‚æ•°å…±äº«æ—¶ä¼šé‡å¤è®¡æ•°ï¼ˆç¬¦åˆPyTorchè®¾è®¡ï¼‰

    ç¤ºä¾‹ç”¨æ³•ï¼š
    =========
    >>> # åŸºæœ¬å‚æ•°ç»Ÿè®¡
    >>> model = torchvision.models.resnet50()
    >>> total, trainable = count_parameters(model)

    >>> # è®­ç»ƒå‰åå¯¹æ¯”
    >>> print("è®­ç»ƒå‰:")
    >>> count_parameters(model)
    >>> # å†»ç»“æŸäº›å±‚...
    >>> print("è®­ç»ƒå:")
    >>> count_parameters(model)

    >>> # è‡ªå®šä¹‰æ¨¡å‹åˆ†æ
    >>> class MyModel(nn.Module):
    ...     def __init__(self):
    ...         super().__init__()
    ...         self.conv = nn.Conv2d(3, 64, 3)
    ...         self.fc = nn.Linear(64, 10)
    >>> model = MyModel()
    >>> count_parameters(model)

    ç›¸å…³æ¦‚å¿µï¼š
    =========
    - FLOPs: æµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼Œä¸å‚æ•°é‡ç›¸å…³ä½†ä¸ç›¸åŒ
    - MACs: ä¹˜ç´¯åŠ è¿ç®—æ¬¡æ•°ï¼Œæ›´å‡†ç¡®çš„å¤æ‚åº¦åº¦é‡
    - Model Size: æ¨¡å‹æ–‡ä»¶å¤§å°ï¼Œå—å‚æ•°ç²¾åº¦å½±å“
    - Memory Usage: è®­ç»ƒæ—¶GPUå†…å­˜å ç”¨

    æ‰©å±•åŠŸèƒ½ï¼š
    =========
    - å¯æ‰©å±•ä¸ºç»Ÿè®¡æ¯å±‚çš„å‚æ•°åˆ†å¸ƒ
    - å¯æ·»åŠ å‚æ•°å¯è§†åŒ–åŠŸèƒ½
    - å¯é›†æˆåˆ°æ¨¡å‹åˆ†æå·¥å…·é“¾ä¸­
    """
    # å‚æ•°ç»Ÿè®¡æ ¸å¿ƒé€»è¾‘
    total_params = 0      # æ€»å‚æ•°è®¡æ•°å™¨
    trainable_params = 0  # å¯è®­ç»ƒå‚æ•°è®¡æ•°å™¨

    # éå†æ¨¡å‹çš„æ‰€æœ‰å‚æ•°
    for param in model.parameters():
        param_count = param.numel()  # è·å–å‚æ•°å¼ é‡çš„å…ƒç´ æ•°é‡
        total_params += param_count

        # æ£€æŸ¥å‚æ•°æ˜¯å¦å¯è®­ç»ƒ
        if param.requires_grad:
            trainable_params += param_count

    # è®¡ç®—å†»ç»“å‚æ•°é‡
    frozen_params = total_params - trainable_params

    # æ ¼å¼åŒ–è¾“å‡ºç»Ÿè®¡ç»“æœ
    print("\nğŸ“Š æ·±åº¦å­¦ä¹ æ¨¡å‹å‚æ•°ç»Ÿè®¡åˆ†æ:")
    print("=" * 50)
    print(f"ğŸ”¢ æ€»å‚æ•°é‡: {total_params:,} ({total_params/1e6:.2f}M)")
    print(f"ğŸ¯ å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/1e6:.2f}M)")
    print(f"â„ï¸  å†»ç»“å‚æ•°: {frozen_params:,}")

    # æä¾›å‚æ•°åˆ†æå»ºè®®
    if total_params > 100_000_000:  # è¶…è¿‡1äº¿å‚æ•°
        print("âš ï¸  æ¨¡å‹å‚æ•°é‡è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦å¤§é‡è®¡ç®—èµ„æº")
    elif total_params < 1_000_000:  # å°‘äº100ä¸‡å‚æ•°
        print("ğŸ’¡ è½»é‡çº§æ¨¡å‹ï¼Œé€‚åˆç§»åŠ¨ç«¯éƒ¨ç½²")

    # å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹åˆ†æ
    if total_params > 0:
        trainable_ratio = trainable_params / total_params
        if trainable_ratio < 0.1:
            print(f"   âš ï¸  å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹è¾ƒä½ ({trainable_ratio:.1f})ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´å†»ç»“ç­–ç•¥")
        elif trainable_ratio > 0.9:
            print(f"   âœ… å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹è¾ƒé«˜ ({trainable_ratio:.1f})ï¼Œé€‚åˆå®Œæ•´å¾®è°ƒ")
    # è¿”å›ç»Ÿè®¡ç»“æœ
    return total_params, trainable_params

def validate_model(model, device, input_size=(1, 3, 640, 640)):
    """
    æ·±åº¦å­¦ä¹ æ¨¡å‹å®Œæ•´æ€§éªŒè¯å‡½æ•°

    é€šè¿‡å…¨é¢çš„å‰å‘ä¼ æ’­æµ‹è¯•éªŒè¯æ¨¡å‹çš„æ¶æ„å®Œæ•´æ€§å’ŒåŠŸèƒ½æ­£ç¡®æ€§ã€‚
    è¯¥å‡½æ•°æ˜¯æ¨¡å‹è°ƒè¯•å’Œéƒ¨ç½²å‰çš„å…³é”®è´¨é‡æ£€æŸ¥å·¥å…·ã€‚

    éªŒè¯ç›®æ ‡ï¼š
    =========
    1. æ¶æ„å®Œæ•´æ€§ (Architecture Integrity)
       - æ£€æŸ¥æ¨¡å‹ç»“æ„æ˜¯å¦å®Œæ•´
       - éªŒè¯æ‰€æœ‰å±‚å’Œè¿æ¥æ˜¯å¦æ­£ç¡®å®šä¹‰

    2. å‰å‘ä¼ æ’­æ­£ç¡®æ€§ (Forward Pass Correctness)
       - æµ‹è¯•æ¨¡å‹æ˜¯å¦èƒ½æ­£å¸¸å¤„ç†è¾“å…¥
       - éªŒè¯è®¡ç®—å›¾çš„å®Œæ•´æ€§

    3. è¾“å‡ºæ ¼å¼æ­£ç¡®æ€§ (Output Format Correctness)
       - æ£€æŸ¥è¾“å‡ºå¼ é‡çš„å½¢çŠ¶å’Œç±»å‹
       - éªŒè¯æ˜¯å¦ç¬¦åˆé¢„æœŸæ ¼å¼

    4. è®¾å¤‡å…¼å®¹æ€§ (Device Compatibility)
       - æµ‹è¯•æ¨¡å‹åœ¨æŒ‡å®šè®¾å¤‡ä¸Šçš„è¿è¡Œèƒ½åŠ›
       - éªŒè¯GPU/CPUå…¼å®¹æ€§

    5. å†…å­˜å®‰å…¨æ€§ (Memory Safety)
       - æ£€æŸ¥æ˜¯å¦å­˜åœ¨å†…å­˜æº¢å‡º
       - éªŒè¯æ˜¾å­˜ä½¿ç”¨æ˜¯å¦åˆç†

    å‚æ•°è¯´æ˜ï¼š
    =========
    model (torch.nn.Module): å¾…éªŒè¯çš„PyTorchæ¨¡å‹
        - æ”¯æŒæ‰€æœ‰PyTorchæ¨¡å‹ç±»å‹
        - åŒ…æ‹¬è‡ªå®šä¹‰æ¨¡å‹å’Œé¢„è®­ç»ƒæ¨¡å‹
        - å¿…é¡»æ˜¯å¯è°ƒç”¨å¯¹è±¡

    device (torch.device): è®¡ç®—è®¾å¤‡
        - cuda: GPUè®¾å¤‡ï¼ˆæ¨èç”¨äºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼‰
        - cpu: CPUè®¾å¤‡ï¼ˆå…¼å®¹æ€§æµ‹è¯•ï¼‰
        - æ”¯æŒè‡ªåŠ¨è®¾å¤‡æ£€æµ‹ç»“æœ

    input_size (tuple): è¾“å…¥å¼ é‡å°ºå¯¸ï¼Œæ ¼å¼(B, C, H, W)
        - B (int): æ‰¹æ¬¡å¤§å°ï¼Œé€šå¸¸ä¸º1ï¼ˆæµ‹è¯•ç”¨ï¼‰
        - C (int): é€šé“æ•°ï¼Œå›¾åƒé€šå¸¸ä¸º3ï¼ˆRGBï¼‰
        - H (int): é«˜åº¦ï¼Œåƒç´ å€¼
        - W (int): å®½åº¦ï¼Œåƒç´ å€¼
        - é»˜è®¤å€¼: (1, 3, 640, 640) - YOLOv8æ ‡å‡†è¾“å…¥

    è¿”å›å€¼ï¼š
    =======
    bool: éªŒè¯ç»“æœ
        - True: æ¨¡å‹éªŒè¯æˆåŠŸï¼Œæ‰€æœ‰æµ‹è¯•é€šè¿‡
        - False: æ¨¡å‹éªŒè¯å¤±è´¥ï¼Œå‘ç°é—®é¢˜

    éªŒè¯æµç¨‹ï¼š
    =========
    1. è®¾å¤‡è¿ç§» (Device Transfer)
       - å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¡ç®—è®¾å¤‡
       - è‡ªåŠ¨å¤„ç†å‚æ•°å’Œç¼“å†²åŒºçš„è®¾å¤‡è¿ç§»

    2. æ¨¡å¼è®¾ç½® (Mode Setting)
       - è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ (model.eval())
       - å…³é—­dropoutå’Œbatch normalizationçš„è®­ç»ƒè¡Œä¸º

    3. è¾“å…¥å‡†å¤‡ (Input Preparation)
       - ç”ŸæˆæŒ‡å®šå°ºå¯¸çš„éšæœºè¾“å…¥å¼ é‡
       - ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¡ç®—è®¾å¤‡

    4. å‰å‘ä¼ æ’­æµ‹è¯• (Forward Pass Test)
       - æ‰§è¡Œæ¨¡å‹æ¨ç†
       - ç¦ç”¨æ¢¯åº¦è®¡ç®—ä»¥èŠ‚çœå†…å­˜

    5. è¾“å‡ºéªŒè¯ (Output Validation)
       - æ£€æŸ¥è¾“å‡ºæ˜¯å¦ä¸ºç©º
       - éªŒè¯è¾“å‡ºå½¢çŠ¶çš„åˆç†æ€§
       - å¤„ç†å¤šè¾“å‡ºæ¨¡å‹çš„æƒ…å†µ

    6. ç»“æœæŠ¥å‘Š (Result Reporting)
       - æˆåŠŸæ—¶æ˜¾ç¤ºè¾“å‡ºå½¢çŠ¶
       - å¤±è´¥æ—¶æä¾›è¯¦ç»†é”™è¯¯ä¿¡æ¯

    æŠ€æœ¯ç»†èŠ‚ï¼š
    =========
    - éšæœºè¾“å…¥: ä½¿ç”¨torch.randnç”Ÿæˆæ ‡å‡†æ­£æ€åˆ†å¸ƒæ•°æ®
    - æ— æ¢¯åº¦ä¸Šä¸‹æ–‡: torch.no_grad()ç¡®ä¿ä¸è®¡ç®—æ¢¯åº¦
    - è¾“å‡ºå¤„ç†: æ”¯æŒå•ä¸ªå¼ é‡å’Œå…ƒç»„/åˆ—è¡¨è¾“å‡º
    - å¼‚å¸¸æ•è·: æ•è·æ‰€æœ‰å¯èƒ½çš„è¿è¡Œæ—¶é”™è¯¯

    ä½¿ç”¨å»ºè®®ï¼š
    =========
    - æ¨¡å‹å¼€å‘å®Œæˆåç«‹å³éªŒè¯
    - æ¨¡å‹ä¿®æ”¹åé‡æ–°éªŒè¯
    - éƒ¨ç½²å‰è¿›è¡Œæœ€ç»ˆéªŒè¯
    - ä¸åŒè®¾å¤‡ä¸Šåˆ†åˆ«æµ‹è¯•

    å¸¸è§é—®é¢˜ï¼š
    =========
    1. CUDAé”™è¯¯: æ£€æŸ¥GPUå†…å­˜æ˜¯å¦å……è¶³
    2. å½¢çŠ¶ä¸åŒ¹é…: éªŒè¯è¾“å…¥å°ºå¯¸æ˜¯å¦æ­£ç¡®
    3. å±‚æœªå®šä¹‰: æ£€æŸ¥æ¨¡å‹åˆå§‹åŒ–æ˜¯å¦å®Œæ•´
    4. æ•°æ®ç±»å‹é”™è¯¯: ç¡®ä¿è¾“å…¥è¾“å‡ºç±»å‹åŒ¹é…

    ç¤ºä¾‹ç”¨æ³•ï¼š
    =========
    >>> # åŸºæœ¬æ¨¡å‹éªŒè¯
    >>> device = setup_device()
    >>> model = MyModel()
    >>> success = validate_model(model, device)

    >>> # æŒ‡å®šè¾“å…¥å°ºå¯¸éªŒè¯
    >>> success = validate_model(model, device, (1, 3, 416, 416))

    >>> # é›†æˆåˆ°è®­ç»ƒæµç¨‹
    >>> model = create_model()
    >>> if validate_model(model, device):
    ...     trainer = Trainer(model, device)
    ...     trainer.train()
    ... else:
    ...     print("æ¨¡å‹éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹å®šä¹‰")

    >>> # æ‰¹é‡éªŒè¯å¤šä¸ªæ¨¡å‹
    >>> models = [model1, model2, model3]
    >>> for i, model in enumerate(models):
    ...     if validate_model(model, device):
    ...         print(f"æ¨¡å‹{i+1}: âœ…")
    ...     else:
    ...         print(f"æ¨¡å‹{i+1}: âŒ")

    æ€§èƒ½å½±å“ï¼š
    =========
    - æ—¶é—´å¼€é”€: é€šå¸¸åœ¨å‡ ç§’åˆ°å‡ åç§’ä¹‹é—´
    - å†…å­˜ä½¿ç”¨: ä¸»è¦æ¶ˆè€—GPU/CPUå†…å­˜ç”¨äºå‰å‘ä¼ æ’­
    - è®¡ç®—èµ„æº: æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„å‰å‘ä¼ æ’­

    æ‰©å±•åŠŸèƒ½ï¼š
    =========
    - å¯æ·»åŠ è¾“å‡ºå€¼èŒƒå›´æ£€æŸ¥
    - å¯é›†æˆæ¢¯åº¦æµæ£€æŸ¥
    - å¯æ·»åŠ æ€§èƒ½åŸºå‡†æµ‹è¯•
    - å¯æ”¯æŒè‡ªå®šä¹‰éªŒè¯è§„åˆ™

    æœ€ä½³å®è·µï¼š
    =========
    1. å¼€å‘é˜¶æ®µ: æ¯æ¬¡ä¿®æ”¹åéªŒè¯
    2. è®­ç»ƒå‰: ç¡®ä¿æ¨¡å‹å¯ç”¨
    3. éƒ¨ç½²å‰: æœ€ç»ˆå®Œæ•´æ€§æ£€æŸ¥
    4. ç”Ÿäº§ç¯å¢ƒ: å®šæœŸå¥åº·æ£€æŸ¥
    """
    try:
        # æ­¥éª¤1: è®¾å¤‡è¿ç§»å’Œæ¨¡å¼è®¾ç½®
        print("ğŸ” å¼€å§‹æ¨¡å‹éªŒè¯æµç¨‹...")
        print(f"   ç›®æ ‡è®¾å¤‡: {device}")
        print(f"   è¾“å…¥å°ºå¯¸: {input_size}")

        # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        model.to(device)

        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œç¡®ä¿æ¨ç†è¡Œä¸ºä¸€è‡´
        model.eval()

        # æ­¥éª¤2: å‡†å¤‡æµ‹è¯•è¾“å…¥
        # ä½¿ç”¨éšæœºæ•°æ®è¿›è¡ŒåŠŸèƒ½æµ‹è¯•
        with torch.no_grad():
            dummy_input = torch.randn(input_size).to(device)
            print(f"   æµ‹è¯•è¾“å…¥å·²å‡†å¤‡: {dummy_input.shape}")

            # æ­¥éª¤3: æ‰§è¡Œå‰å‘ä¼ æ’­
            print("   æ‰§è¡Œå‰å‘ä¼ æ’­æµ‹è¯•...")
            output = model(dummy_input)

            # æ­¥éª¤4: éªŒè¯è¾“å‡º
            if output is None:
                raise ValueError("æ¨¡å‹è¾“å‡ºä¸ºç©º")

            # å¤„ç†ä¸åŒç±»å‹çš„è¾“å‡º
            if isinstance(output, (list, tuple)):
                # å¤šè¾“å‡ºæ¨¡å‹ï¼ˆå¦‚YOLOç³»åˆ—ï¼‰
                output_shape = output[0].shape
                num_outputs = len(output)
                print(f"   å¤šè¾“å‡ºæ¨¡å‹: {num_outputs}ä¸ªè¾“å‡ºåˆ†æ”¯")
            else:
                # å•è¾“å‡ºæ¨¡å‹
                output_shape = output.shape
                print(f"   å•è¾“å‡ºæ¨¡å‹")

            # éªŒè¯è¾“å‡ºå½¢çŠ¶çš„åˆç†æ€§
            if len(output_shape) < 2:
                raise ValueError(f"è¾“å‡ºç»´åº¦è¿‡ä½: {output_shape}")

            # è¾“å‡ºéªŒè¯æˆåŠŸä¿¡æ¯
            print("âœ… æ¨¡å‹éªŒè¯æˆåŠŸï¼")
            print(f"   è¾“å‡ºå½¢çŠ¶: {output_shape}")
            print(f"   è¾“å‡ºç±»å‹: {type(output)}")
            print(f"   æ•°æ®ç±»å‹: {output.dtype if hasattr(output, 'dtype') else 'N/A'}")

            # é¢å¤–çš„è¾“å‡ºåˆ†æ
            if hasattr(output, 'shape') and len(output.shape) >= 2:
                batch_size, channels = output.shape[0], output.shape[1]
                print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
                if len(output.shape) > 2:
                    spatial_dims = ' Ã— '.join(str(s) for s in output.shape[2:])
                    print(f"   ç©ºé—´ç»´åº¦: {spatial_dims}")

        return True

    except Exception as e:
        # è¯¦ç»†çš„é”™è¯¯æŠ¥å‘Š
        print("âŒ æ¨¡å‹éªŒè¯å¤±è´¥ï¼")
        print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")

        # æä¾›å¯èƒ½çš„è§£å†³æ–¹æ¡ˆå»ºè®®
        error_msg = str(e).lower()
        if 'cuda' in error_msg:
            print("   ğŸ’¡ å»ºè®®: æ£€æŸ¥GPUå†…å­˜æ˜¯å¦å……è¶³ï¼Œæˆ–å°è¯•ä½¿ç”¨CPU")
        elif 'shape' in error_msg:
            print("   ğŸ’¡ å»ºè®®: æ£€æŸ¥è¾“å…¥å°ºå¯¸æ˜¯å¦ä¸æ¨¡å‹æœŸæœ›åŒ¹é…")
        elif 'device' in error_msg:
            print("   ğŸ’¡ å»ºè®®: ç¡®ä¿æ¨¡å‹å’Œè¾“å…¥åœ¨åŒä¸€è®¾å¤‡ä¸Š")
        else:
            print("   ğŸ’¡ å»ºè®®: æ£€æŸ¥æ¨¡å‹å®šä¹‰å’Œåˆå§‹åŒ–æ˜¯å¦æ­£ç¡®")

        return False

@smart_inference_mode()
def benchmark_inference(model, device, input_size=(1, 3, 640, 640), num_runs=100):
    """
    æ·±åº¦å­¦ä¹ æ¨¡å‹æ¨ç†æ€§èƒ½åŸºå‡†æµ‹è¯•å‡½æ•°

    å¯¹æ¨¡å‹è¿›è¡Œå…¨é¢çš„æ¨ç†æ€§èƒ½è¯„ä¼°ï¼Œæµ‹é‡åœ¨æŒ‡å®šè®¾å¤‡ä¸Šçš„é€Ÿåº¦ã€ååé‡å’Œå»¶è¿Ÿã€‚
    è¯¥å‡½æ•°æ˜¯æ¨¡å‹éƒ¨ç½²å’Œä¼˜åŒ–å‰çš„å…³é”®æ€§èƒ½åˆ†æå·¥å…·ã€‚

    æµ‹è¯•ç›®æ ‡ï¼š
    =========
    1. æ¨ç†é€Ÿåº¦æµ‹é‡ (Inference Speed)
       - å•æ¬¡æ¨ç†çš„å¹³å‡è€—æ—¶
       - ååé‡(FPS)å’Œå»¶è¿Ÿ(ms)æŒ‡æ ‡

    2. æ€§èƒ½ç¨³å®šæ€§è¯„ä¼° (Performance Stability)
       - å¤šæ¬¡è¿è¡Œçš„æ€§èƒ½ä¸€è‡´æ€§
       - é¢„çƒ­åçš„ç¨³å®šæ€§èƒ½è¡¨ç°

    3. è®¾å¤‡æ€§èƒ½å¯¹æ¯” (Device Performance Comparison)
       - GPU vs CPUæ€§èƒ½å·®å¼‚
       - ç¡¬ä»¶åŠ é€Ÿæ•ˆæœé‡åŒ–

    4. å†…å­˜ä½¿ç”¨åˆ†æ (Memory Usage Analysis)
       - GPUæ˜¾å­˜å ç”¨æƒ…å†µ
       - å†…å­˜ä½¿ç”¨æ•ˆç‡è¯„ä¼°

    å‚æ•°è¯´æ˜ï¼š
    =========
    model (torch.nn.Module): å¾…æµ‹è¯•çš„PyTorchæ¨¡å‹
        - å¿…é¡»æ˜¯å·²åˆå§‹åŒ–å¹¶å¯è¿›è¡Œæ¨ç†çš„æ¨¡å‹
        - æ”¯æŒæ‰€æœ‰PyTorchæ¨¡å‹ç±»å‹

    device (torch.device): è®¡ç®—è®¾å¤‡
        - cuda: GPUè®¾å¤‡ï¼Œç”¨äºé«˜é€Ÿå¹¶è¡Œè®¡ç®—
        - cpu: CPUè®¾å¤‡ï¼Œç”¨äºå…¼å®¹æ€§æµ‹è¯•
        - å½±å“æµ‹è¯•ç»“æœå’Œæ€§èƒ½æŒ‡æ ‡

    input_size (tuple): è¾“å…¥å¼ é‡å°ºå¯¸ï¼Œæ ¼å¼(B, C, H, W)
        - B: æ‰¹æ¬¡å¤§å°ï¼Œé€šå¸¸è®¾ä¸º1ï¼ˆæ¨¡æ‹Ÿå•å¼ å›¾ç‰‡æ¨ç†ï¼‰
        - C: é€šé“æ•°ï¼Œå›¾åƒé€šå¸¸ä¸º3
        - H, W: ç©ºé—´å°ºå¯¸
        - é»˜è®¤: (1, 3, 640, 640) - YOLOv8æ ‡å‡†è¾“å…¥

    num_runs (int): æµ‹è¯•è¿è¡Œæ¬¡æ•°ï¼Œé»˜è®¤100
        - ç»Ÿè®¡å­¦ä¸Šéœ€è¦è¶³å¤Ÿæ ·æœ¬ä¿è¯ç»“æœå¯é æ€§
        - å»ºè®®å€¼: 50-500ï¼Œæ ¹æ®æ¨¡å‹å¤æ‚åº¦è°ƒæ•´
        - å½±å“æµ‹è¯•æ—¶é•¿å’Œç»“æœç¨³å®šæ€§

    è¿”å›å€¼ï¼š
    =======
    tuple: (å¹³å‡æ¨ç†æ—¶é—´ms, å¸§ç‡FPS)
        - avg_time (float): å•æ¬¡æ¨ç†å¹³å‡è€—æ—¶ï¼Œå•ä½æ¯«ç§’
        - fps (float): æ¯ç§’å¤„ç†çš„å¸§æ•°ï¼Œå¸§ç‡æŒ‡æ ‡

    æµ‹è¯•æ–¹æ³•è®ºï¼š
    ===========
    1. é¢„çƒ­é˜¶æ®µ (Warm-up Phase)
       - è¿è¡Œ10æ¬¡æ¨ç†ä»¥è¾¾åˆ°ç¨³å®šæ€§èƒ½çŠ¶æ€
       - æ¶ˆé™¤å†·å¯åŠ¨å¼€é”€å’Œç¼“å­˜æ•ˆåº”
       - ç¡®ä¿GPUé¢‘ç‡ç¨³å®š

    2. åŸºå‡†æµ‹è¯•é˜¶æ®µ (Benchmark Phase)
       - è¿è¡ŒæŒ‡å®šæ¬¡æ•°(num_runs)çš„æ¨ç†
       - ç²¾ç¡®è®¡æ—¶æ¯ä¸ªé˜¶æ®µçš„è€—æ—¶
       - GPUç¯å¢ƒä¸‹è¿›è¡Œæ˜¾å¼åŒæ­¥

    3. æ€§èƒ½è®¡ç®— (Performance Calculation)
       - å¹³å‡æ—¶é—´: æ€»æ—¶é—´/è¿è¡Œæ¬¡æ•° * 1000ï¼ˆmsï¼‰
       - å¸§ç‡: 1000/å¹³å‡æ—¶é—´ï¼ˆFPSï¼‰

    4. ç»“æœåˆ†æ (Result Analysis)
       - æä¾›è¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Š
       - åŒ…å«è®¾å¤‡ä¿¡æ¯å’Œæµ‹è¯•æ¡ä»¶

    æŠ€æœ¯å®ç°ï¼š
    =========
    - é¢„çƒ­ç­–ç•¥: å›ºå®š10æ¬¡é¢„çƒ­è¿è¡Œ
    - æ—¶é—´æµ‹é‡: ä½¿ç”¨time.time()é«˜ç²¾åº¦è®¡æ—¶
    - GPUåŒæ­¥: torch.cuda.synchronize()ç¡®ä¿æ“ä½œå®Œæˆ
    - å†…å­˜ä¼˜åŒ–: torch.no_grad()ç¦ç”¨æ¢¯åº¦è®¡ç®—
    - éšæœºè¾“å…¥: torch.randnç”Ÿæˆæµ‹è¯•æ•°æ®

    æ€§èƒ½æŒ‡æ ‡è§£é‡Šï¼š
    =============
    1. å¹³å‡æ¨ç†æ—¶é—´ (Average Inference Time)
       - å•å¼ å›¾ç‰‡çš„å¤„ç†æ—¶é—´
       - è¶Šå°è¶Šå¥½ï¼Œåæ˜ æ¨¡å‹æ•ˆç‡
       - å•ä½: æ¯«ç§’(ms)

    2. å¸§ç‡ (Frames Per Second - FPS)
       - æ¯ç§’å¯å¤„ç†çš„å›¾ç‰‡æ•°é‡
       - å®æ—¶åº”ç”¨ä¸­çš„é‡è¦æŒ‡æ ‡
       - 30 FPSä»¥ä¸Šé€‚åˆå®æ—¶åº”ç”¨

    3. ååé‡ (Throughput)
       - batch_size > 1æ—¶çš„ç»¼åˆæ€§èƒ½
       - åæ˜ æ¨¡å‹çš„å¹¶è¡Œå¤„ç†èƒ½åŠ›

    ä½¿ç”¨å»ºè®®ï¼š
    =========
    - æµ‹è¯•å‰ç¡®ä¿æ¨¡å‹å·²åŠ è½½åˆ°æ­£ç¡®è®¾å¤‡
    - GPUæµ‹è¯•éœ€è¦é¢„ç•™è¶³å¤Ÿæ˜¾å­˜
    - å¤šæ¬¡æµ‹è¯•å–å¹³å‡å€¼ä»¥ä¿è¯å¯é æ€§
    - å¯¹æ¯”ä¸åŒæ¨¡å‹å’Œè®¾å¤‡çš„æ€§èƒ½

    æœ€ä½³å®è·µï¼š
    =========
    >>> # æ ‡å‡†æ€§èƒ½æµ‹è¯•
    >>> device = setup_device()
    >>> model = model.to(device)
    >>> avg_time, fps = benchmark_inference(model, device)

    >>> # è‡ªå®šä¹‰æµ‹è¯•å‚æ•°
    >>> avg_time, fps = benchmark_inference(
    ...     model, device,
    ...     input_size=(1, 3, 416, 416),
    ...     num_runs=200
    ... )

    >>> # æ‰¹é‡æµ‹è¯•å¤šä¸ªæ¨¡å‹
    >>> models = {
    ...     'YOLOv8n': yolov8n,
    ...     'YOLOv8s': yolov8s,
    ...     'YOLOv8m': yolov8m
    ... }
    >>> results = {}
    >>> for name, model in models.items():
    ...     model = model.to(device)
    ...     avg_time, fps = benchmark_inference(model, device)
    ...     results[name] = {'time': avg_time, 'fps': fps}

    æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼š
    =============
    - GPUåˆ©ç”¨ç‡ä½: è€ƒè™‘å¢å¤§batch_size
    - æ¨ç†æ—¶é—´é•¿: è€ƒè™‘æ¨¡å‹å‰ªææˆ–é‡åŒ–
    - å†…å­˜ä¸è¶³: å‡å°è¾“å…¥å°ºå¯¸æˆ–ä½¿ç”¨CPU
    - FPSä¸ç¨³å®š: æ£€æŸ¥GPUé¢‘ç‡å’Œå†·å´

    ç»“æœè§£è¯»ï¼š
    =========
    - < 10ms: æé«˜æ€§èƒ½ï¼Œé€‚åˆé«˜å®æ—¶æ€§åº”ç”¨
    - 10-50ms: è‰¯å¥½æ€§èƒ½ï¼Œé€‚åˆå¤§å¤šæ•°å®æ—¶åº”ç”¨
    - 50-200ms: ä¸€èˆ¬æ€§èƒ½ï¼Œé€‚åˆç¦»çº¿å¤„ç†
    - > 200ms: æ€§èƒ½è¾ƒæ…¢ï¼Œéœ€è¦ä¼˜åŒ–

    FPSç›®æ ‡:
    - å®æ—¶è§†é¢‘: > 30 FPS
    - å¿«é€Ÿæ£€æµ‹: > 10 FPS
    - ç¦»çº¿å¤„ç†: > 1 FPS

    æŠ€æœ¯æ³¨æ„äº‹é¡¹ï¼š
    =============
    - GPUæµ‹è¯•åŒ…å«æ˜¾å¼åŒæ­¥ï¼Œå¯èƒ½ç•¥å¾®å½±å“ç»“æœ
    - CPUæµ‹è¯•åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹å¯èƒ½æœ‰æ³¢åŠ¨
    - è¾“å…¥å°ºå¯¸å½±å“æ€§èƒ½ï¼Œä¸åŒåˆ†è¾¨ç‡ç»“æœä¸åŒ
    - æ¨¡å‹å¤æ‚åº¦ç›´æ¥å½±å“æ¨ç†æ—¶é—´

    æ‰©å±•åŠŸèƒ½ï¼š
    =========
    - å¯æ·»åŠ å†…å­˜ä½¿ç”¨é‡ç»Ÿè®¡
    - å¯é›†æˆGPUåˆ©ç”¨ç‡ç›‘æ§
    - å¯æ”¯æŒbatch_size > 1çš„æµ‹è¯•
    - å¯æ·»åŠ æ€§èƒ½å¯¹æ¯”å›¾è¡¨ç”Ÿæˆ
    """
    # åˆå§‹åŒ–å’Œå‡†å¤‡é˜¶æ®µ
    print("ğŸ å¼€å§‹æ·±åº¦å­¦ä¹ æ¨¡å‹æ¨ç†æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print(f"   ç›®æ ‡è®¾å¤‡: {device}")
    print(f"   è¾“å…¥å°ºå¯¸: {input_size}")
    print(f"   æµ‹è¯•è½®æ•°: {num_runs}")

    # æ­¥éª¤1: æ¨¡å‹å‡†å¤‡å’Œè®¾å¤‡è¿ç§»
    model.to(device)
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

    # æ­¥éª¤2: é¢„çƒ­é˜¶æ®µ - æ¶ˆé™¤å†·å¯åŠ¨å¼€é”€
    print("\nğŸ”¥ æ‰§è¡Œé¢„çƒ­é˜¶æ®µ...")
    print("   è¿è¡Œ10æ¬¡æ¨ç†ä»¥è¾¾åˆ°ç¨³å®šæ€§èƒ½çŠ¶æ€")

    with torch.no_grad():
        for i in range(10):
            dummy_input = torch.randn(input_size).to(device)
            _ = model(dummy_input)
            if (i + 1) % 5 == 0:
                print(f"   é¢„çƒ­è¿›åº¦: {i + 1}/10")

    print("   âœ… é¢„çƒ­å®Œæˆï¼Œæ€§èƒ½å·²ç¨³å®š")

    # æ­¥éª¤3: åŸºå‡†æµ‹è¯•é˜¶æ®µ
    import time

    # GPUåŒæ­¥å‡†å¤‡ï¼ˆç¡®ä¿å‡†ç¡®è®¡æ—¶ï¼‰
    if device.type == 'cuda':
        torch.cuda.synchronize()
        print("   ğŸ“Š GPUåŒæ­¥å®Œæˆï¼Œå¼€å§‹ç²¾ç¡®è®¡æ—¶")

    print(f"\nâ±ï¸  å¼€å§‹åŸºå‡†æµ‹è¯•...")
    print(f"   å°†æ‰§è¡Œ{num_runs}æ¬¡æ¨ç†æµ‹è¯•")

    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()

    # æ‰§è¡Œå¤šæ¬¡æ¨ç†æµ‹è¯•
    with torch.no_grad():
        for run in range(num_runs):
            dummy_input = torch.randn(input_size).to(device)
            _ = model(dummy_input)

            # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯10%æ˜¾ç¤ºä¸€æ¬¡ï¼‰
            if num_runs >= 10 and (run + 1) % (num_runs // 10) == 0:
                progress = (run + 1) / num_runs * 100
                print(f"   æ¨ç†è¿›åº¦: {progress:.1f}%")
    # GPUåŒæ­¥ç»“æŸï¼ˆç¡®ä¿æ‰€æœ‰æ“ä½œå®Œæˆï¼‰
    if device.type == 'cuda':
        torch.cuda.synchronize()

    # è®¡ç®—æ€»è€—æ—¶
    end_time = time.time()
    total_time_seconds = end_time - start_time

    # æ­¥éª¤4: æ€§èƒ½æŒ‡æ ‡è®¡ç®—
    avg_time_ms = (total_time_seconds / num_runs) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
    fps = 1000 / avg_time_ms  # è®¡ç®—å¸§ç‡

    # æ­¥éª¤5: è¯¦ç»†ç»“æœå±•ç¤º
    print("\nğŸ¯ æ¨ç†æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ:")
    print("=" * 50)
    print("ğŸ“ˆ æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡:")
    print(f"   ğŸ• å¹³å‡æ¨ç†æ—¶é—´: {avg_time_ms:.2f} ms")
    print(f"   ğŸ¬ å®æ—¶å¸§ç‡ (FPS): {fps:.2f}")
    print(f"   ğŸ”„ æµ‹è¯•æ€»æ—¶é—´: {total_time_seconds:.2f} ç§’")
    print(f"   ğŸ“Š æ‰§è¡Œæ¨ç†æ¬¡æ•°: {num_runs}")

    print("\nğŸ’» æµ‹è¯•ç¯å¢ƒä¿¡æ¯:")
    print(f"   ğŸ–¥ï¸  è®¡ç®—è®¾å¤‡: {device}")
    print(f"   ğŸ“ è¾“å…¥å¼ é‡å°ºå¯¸: {input_size}")
    print(f"   ğŸ§  æ¨¡å‹ç±»å‹: {type(model).__name__}")

    # æ€§èƒ½ç­‰çº§è¯„ä¼°
    if fps >= 30:
        performance_level = "ğŸš€ æé«˜æ€§èƒ½ - é€‚åˆå®æ—¶è§†é¢‘å¤„ç†"
    elif fps >= 10:
        performance_level = "âš¡ é«˜æ€§èƒ½ - é€‚åˆå®æ—¶åº”ç”¨"
    elif fps >= 1:
        performance_level = "ğŸ“ˆ è‰¯å¥½æ€§èƒ½ - é€‚åˆå¿«é€Ÿæ£€æµ‹"
    else:
        performance_level = "ğŸŒ åŸºç¡€æ€§èƒ½ - é€‚åˆç¦»çº¿å¤„ç†"

    print(f"\nğŸ† æ€§èƒ½ç­‰çº§è¯„ä¼°: {performance_level}")

    # ä¼˜åŒ–å»ºè®®
    print("\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
    if device.type == 'cuda':
        if fps < 10:
            print("   â€¢ è€ƒè™‘ä½¿ç”¨æ›´å¤§çš„batch_sizeæé«˜GPUåˆ©ç”¨ç‡")
            print("   â€¢ æ£€æŸ¥æ˜¯å¦å¯ä»¥åº”ç”¨æ¨¡å‹å‰ªææˆ–é‡åŒ–")
        else:
            print("   â€¢ GPUæ€§èƒ½è‰¯å¥½ï¼Œå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹ç»“æ„")
    else:
        print("   â€¢ å½“å‰ä½¿ç”¨CPUï¼Œè€ƒè™‘å‡çº§åˆ°GPUä»¥è·å¾—æ›´å¥½æ€§èƒ½")
        if fps < 1:
            print("   â€¢ è€ƒè™‘ä½¿ç”¨æ›´è½»é‡çº§çš„æ¨¡å‹æ¶æ„")

    print("   â€¢ å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿›è¡Œå¤šè½®æµ‹è¯•ä»¥ç¡®ä¿ç¨³å®šæ€§")

    # è¿”å›æ€§èƒ½æŒ‡æ ‡
    return avg_time_ms, fps
