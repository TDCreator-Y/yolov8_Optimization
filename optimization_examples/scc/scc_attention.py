# SCC (ç©ºé—´-é€šé“äº¤å‰)æ³¨æ„åŠ›å®ç°
# ===========================

"""
YOLOv8çš„SCCæ³¨æ„åŠ›æ¨¡å—

æœ¬æ¨¡å—å®ç°ç©ºé—´-é€šé“äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œ
ç»“åˆç©ºé—´æ³¨æ„åŠ›å’Œé€šé“æ³¨æ„åŠ›ä»¥å¢å¼ºç‰¹å¾è¡¨ç¤ºã€‚

åŸºäºCBAM (Convolutional Block Attention Module)æ¶æ„ã€‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class ChannelAttention(nn.Module):
    """
    é€šé“æ³¨æ„åŠ›æ¨¡å—

    åŸºäºå…¨å±€å¹³å‡æ± åŒ–ç»Ÿè®¡ä¿¡æ¯ï¼Œåœ¨é€šé“ç»´åº¦ä¸Šåº”ç”¨æ³¨æ„åŠ›æƒé‡ã€‚

    å·¥ä½œåŸç†ï¼š
        - é€šè¿‡å…¨å±€å¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–è·å–é€šé“ç»Ÿè®¡ä¿¡æ¯
        - ä½¿ç”¨MLPå­¦ä¹ é€šé“é—´çš„ç›¸å…³æ€§å’Œé‡è¦æ€§
        - ç”Ÿæˆé€šé“æ³¨æ„åŠ›æƒé‡è¿›è¡Œç‰¹å¾é‡æ ‡å®š

    æŠ€æœ¯ç‰¹ç‚¹ï¼š
        - æ•è·é€šé“é—´çš„å…¨å±€ä¾èµ–å…³ç³»
        - è½»é‡çº§è®¡ç®—ï¼Œå‚æ•°æ•ˆç‡é«˜
        - è‡ªé€‚åº”å­¦ä¹ ä¸åŒé€šé“çš„é‡è¦æ€§
    """

    def __init__(self, channels: int, reduction: int = 16):
        """
        åˆå§‹åŒ–é€šé“æ³¨æ„åŠ›æ¨¡å—

        å‚æ•°ï¼š
            channels (int): è¾“å…¥é€šé“æ•°
            reduction (int): é€šé“å‹ç¼©æ¯”ä¾‹ï¼Œé»˜è®¤16

        ç½‘ç»œç»“æ„ï¼š
            è¾“å…¥ -> å…¨å±€å¹³å‡æ± åŒ–/æœ€å¤§æ± åŒ– -> MLP -> Sigmoid -> æ³¨æ„åŠ›æƒé‡
            â”‚                                      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        è®¾è®¡ç†å¿µï¼š
            - é€šè¿‡é™ç»´MLPå‡å°‘å‚æ•°é‡
            - åŒæ—¶ä½¿ç”¨å¹³å‡å’Œæœ€å¤§æ± åŒ–è·å–æ›´ä¸°å¯Œçš„ç»Ÿè®¡ä¿¡æ¯
            - Sigmoidæ¿€æ´»ç”Ÿæˆ0-1ä¹‹é—´çš„æ³¨æ„åŠ›æƒé‡
        """
        super().__init__()

        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        self.fc = nn.Sequential(
            nn.Conv2d(channels, channels // reduction, 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(channels // reduction, channels, 1, bias=False)
        )

        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        """
        é€šé“æ³¨æ„åŠ›å‰å‘ä¼ æ’­

        å‚æ•°ï¼š
            x (torch.Tensor): è¾“å…¥ç‰¹å¾å›¾ï¼Œå½¢çŠ¶ä¸º[B, C, H, W]

        è¿”å›å€¼ï¼š
            torch.Tensor: é€šé“æ³¨æ„åŠ›åŠ æƒåçš„ç‰¹å¾å›¾ï¼Œå½¢çŠ¶ä¸º[B, C, H, W]
        """
        # å¹¶è¡Œå¤„ç†å¹³å‡å’Œæœ€å¤§æ± åŒ–ç»“æœ
        avg_out = self.fc(self.avg_pool(x))  # [B, C, 1, 1]
        max_out = self.fc(self.max_pool(x))  # [B, C, 1, 1]

        # èåˆä¸¤ç§æ± åŒ–ç»“æœå¹¶ç”Ÿæˆæ³¨æ„åŠ›æƒé‡
        attention = self.sigmoid(avg_out + max_out)  # [B, C, 1, 1]

        # åº”ç”¨æ³¨æ„åŠ›æƒé‡ï¼ˆå¹¿æ’­æœºåˆ¶ï¼‰
        return x * attention


class SpatialAttention(nn.Module):
    """
    ç©ºé—´æ³¨æ„åŠ›æ¨¡å—

    åŸºäºé€šé“ç»Ÿè®¡ä¿¡æ¯ï¼Œåœ¨ç©ºé—´ç»´åº¦ä¸Šåº”ç”¨æ³¨æ„åŠ›æƒé‡ã€‚

    å·¥ä½œåŸç†ï¼š
        - åœ¨é€šé“ç»´åº¦ä¸Šè¿›è¡Œå¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–
        - ä½¿ç”¨å·ç§¯å­¦ä¹ ç©ºé—´ä½ç½®çš„é‡è¦æ€§
        - ç”Ÿæˆç©ºé—´æ³¨æ„åŠ›æƒé‡è¿›è¡Œç‰¹å¾é‡æ ‡å®š

    æŠ€æœ¯ç‰¹ç‚¹ï¼š
        - æ•è·ç©ºé—´ä½ç½®é—´çš„ä¾èµ–å…³ç³»
        - 7x7å·ç§¯æ„Ÿå—é‡è¦†ç›–æ›´å¤§ç©ºé—´èŒƒå›´
        - è½»é‡çº§è®¾è®¡ï¼Œè®¡ç®—æ•ˆç‡é«˜
    """

    def __init__(self, kernel_size: int = 7):
        """
        åˆå§‹åŒ–ç©ºé—´æ³¨æ„åŠ›æ¨¡å—

        å‚æ•°ï¼š
            kernel_size (int): å·ç§¯æ ¸å°ºå¯¸ï¼ˆé€šå¸¸ä¸º3æˆ–7ï¼‰ï¼Œé»˜è®¤7

        ç½‘ç»œç»“æ„ï¼š
            è¾“å…¥ -> é€šé“å¹³å‡/æœ€å¤§æ± åŒ– -> æ‹¼æ¥ -> å·ç§¯ -> Sigmoid -> æ³¨æ„åŠ›æƒé‡
            â”‚                                             â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        è®¾è®¡ç†å¿µï¼š
            - é€šé“æ± åŒ–èšåˆå…¨å±€é€šé“ä¿¡æ¯
            - å·ç§¯æ“ä½œå»ºæ¨¡ç©ºé—´ä½ç½®å…³ç³»
            - å¤§å·ç§¯æ ¸æ•è·æ›´å¤§èŒƒå›´çš„ç©ºé—´ä¾èµ–
        """
        super().__init__()

        # å‚æ•°éªŒè¯
        assert kernel_size in {3, 7}, "kernel_sizeå¿…é¡»æ˜¯3æˆ–7"
        padding = 3 if kernel_size == 7 else 1

        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        """
        Args:
            x: input tensor [B, C, H, W]

        Returns:
            spatially-attended tensor [B, C, H, W]
        """
        # Compute channel-wise statistics
        avg_out = torch.mean(x, dim=1, keepdim=True)  # [B, 1, H, W]
        max_out = torch.max(x, dim=1, keepdim=True)[0]  # [B, 1, H, W]

        # Concatenate and apply convolution
        attention = self.conv(torch.cat([avg_out, max_out], dim=1))  # [B, 1, H, W]
        attention = self.sigmoid(attention)

        return x * attention


class SCC_Attention(nn.Module):
    """
    ç©ºé—´-é€šé“äº¤å‰ (SCC) æ³¨æ„åŠ›æ¨¡å—

    ä¾æ¬¡ç»“åˆé€šé“æ³¨æ„åŠ›å’Œç©ºé—´æ³¨æ„åŠ›è¿›è¡Œå…¨é¢çš„ç‰¹å¾é‡æ ‡å®šã€‚

    è¿™æœ¬è´¨ä¸Šæ˜¯CBAM (Convolutional Block Attention Module)çš„å®ç°ã€‚

    å·¥ä½œæµç¨‹ï¼š
        1. å…ˆåº”ç”¨é€šé“æ³¨æ„åŠ›ï¼šå­¦ä¹ å“ªäº›é€šé“æ›´é‡è¦
        2. å†åº”ç”¨ç©ºé—´æ³¨æ„åŠ›ï¼šå­¦ä¹ å“ªäº›ç©ºé—´ä½ç½®æ›´é‡è¦
        3. é¡ºåºå¤„ç†å®ç°é€šé“å’Œç©ºé—´çš„ååŒä¼˜åŒ–

    æŠ€æœ¯ç‰¹ç‚¹ï¼š
        - åŒé‡æ³¨æ„åŠ›æœºåˆ¶ï¼šé€šé“çº§å’Œç©ºé—´çº§çš„äº’è¡¥
        - é¡ºåºå¤„ç†ï¼šé€šé“ä¼˜å…ˆï¼Œç„¶åç©ºé—´ç»†åŒ–
        - å³æ’å³ç”¨ï¼šæ— ç¼é›†æˆåˆ°ä»»ä½•å·ç§¯ç½‘ç»œä¸­
    """

    def __init__(self, channels: int, reduction: int = 16, spatial_kernel: int = 7):
        """
        åˆå§‹åŒ–SCCæ³¨æ„åŠ›æ¨¡å—

        å‚æ•°ï¼š
            channels (int): è¾“å…¥é€šé“æ•°
            reduction (int): é€šé“æ³¨æ„åŠ›çš„å‹ç¼©æ¯”ä¾‹ï¼Œé»˜è®¤16
            spatial_kernel (int): ç©ºé—´æ³¨æ„åŠ›çš„å·ç§¯æ ¸å°ºå¯¸ï¼Œé»˜è®¤7

        ç½‘ç»œç»“æ„ï¼š
            è¾“å…¥ -> é€šé“æ³¨æ„åŠ› -> ç©ºé—´æ³¨æ„åŠ› -> è¾“å‡º
                    â†“              â†“
               å­¦ä¹ é€šé“é‡è¦æ€§  å­¦ä¹ ç©ºé—´é‡è¦æ€§

        è®¾è®¡ç†å¿µï¼š
            - é€šé“æ³¨æ„åŠ›å¤„ç†"å…¨å±€ä»€ä¹ˆé‡è¦"
            - ç©ºé—´æ³¨æ„åŠ›å¤„ç†"å±€éƒ¨å“ªé‡Œé‡è¦"
            - é¡ºåºåº”ç”¨å®ç°å±‚æ¬¡åŒ–ç‰¹å¾ä¼˜åŒ–
        """
        super().__init__()

        # åˆå§‹åŒ–ä¸¤ä¸ªå­æ³¨æ„åŠ›æ¨¡å—
        self.channel_attention = ChannelAttention(channels, reduction)
        self.spatial_attention = SpatialAttention(spatial_kernel)

    def forward(self, x):
        """
        é¡ºåºåº”ç”¨SCCæ³¨æ„åŠ›

        å‚æ•°ï¼š
            x (torch.Tensor): è¾“å…¥ç‰¹å¾å›¾ï¼Œå½¢çŠ¶ä¸º[B, C, H, W]

        è¿”å›å€¼ï¼š
            torch.Tensor: æ³¨æ„åŠ›åŠ æƒåçš„ç‰¹å¾å›¾ï¼Œå½¢çŠ¶ä¸º[B, C, H, W]

        å¤„ç†æµç¨‹ï¼š
            1. å…ˆåº”ç”¨é€šé“æ³¨æ„åŠ›ï¼šé‡æ ‡å®šé€šé“é‡è¦æ€§
            2. å†åº”ç”¨ç©ºé—´æ³¨æ„åŠ›ï¼šé‡æ ‡å®šç©ºé—´é‡è¦æ€§
            3. è¿”å›ä¼˜åŒ–åçš„ç‰¹å¾è¡¨ç¤º

        æŠ€æœ¯ç»†èŠ‚ï¼š
            - é€šé“ä¼˜å…ˆï¼šå…ˆç¡®å®šå“ªäº›ç‰¹å¾é€šé“æ›´é‡è¦
            - ç©ºé—´ç»†åŒ–ï¼šåœ¨é‡è¦é€šé“çš„åŸºç¡€ä¸Šä¼˜åŒ–ç©ºé—´ä½ç½®
            - é¡ºåºå¤„ç†ï¼šå®ç°é€šé“å’Œç©ºé—´çš„ååŒä¼˜åŒ–
        """
        # å…ˆåº”ç”¨é€šé“æ³¨æ„åŠ›ï¼Œå†åº”ç”¨ç©ºé—´æ³¨æ„åŠ›
        x = self.channel_attention(x)  # é€šé“ç»´åº¦é‡æ ‡å®š
        x = self.spatial_attention(x)  # ç©ºé—´ç»´åº¦é‡æ ‡å®š
        return x


class SCC_Bottleneck(nn.Module):
    """
    SCCå¢å¼ºçš„ç“¶é¢ˆå—ï¼Œé›†æˆæ³¨æ„åŠ›æœºåˆ¶

    åœ¨æ ‡å‡†ç“¶é¢ˆå—ä¸­é›†æˆSCCæ³¨æ„åŠ›æœºåˆ¶ã€‚

    æ¶æ„ç‰¹ç‚¹ï¼š
        - ä¿æŒæ ‡å‡†ç“¶é¢ˆå—çš„ç»“æ„ï¼ˆ1x1 -> 3x3 -> 1x1ï¼‰
        - åœ¨3x3å·ç§¯åæ·»åŠ SCCæ³¨æ„åŠ›æ¨¡å—
        - å¯é€‰çš„æ®‹å·®è¿æ¥ä¿æŒæ¢¯åº¦æµåŠ¨
        - è½»é‡çº§æ³¨æ„åŠ›å¢å¼º

    æŠ€æœ¯ä¼˜åŠ¿ï¼š
        - æ³¨æ„åŠ›å¢å¼ºï¼šæå‡ç‰¹å¾è´¨é‡å’Œè¡¨è¾¾èƒ½åŠ›
        - ç»“æ„ä¿æŒï¼šä¸æ ‡å‡†ç“¶é¢ˆå—å®Œå…¨å…¼å®¹
        - è®¡ç®—å‡è¡¡ï¼šæ³¨æ„åŠ›å¼€é”€ä¸æ€§èƒ½æå‡çš„è‰¯å¥½å¹³è¡¡
    """

    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5, use_scc=True):
        """
        åˆå§‹åŒ–SCCå¢å¼ºçš„ç“¶é¢ˆå—

        å‚æ•°ï¼š
            c1 (int): è¾“å…¥é€šé“æ•°
            c2 (int): è¾“å‡ºé€šé“æ•°
            shortcut (bool): æ˜¯å¦ä½¿ç”¨æ®‹å·®è¿æ¥ï¼Œé»˜è®¤True
            g (int): å·ç§¯åˆ†ç»„æ•°ï¼Œé»˜è®¤1
            e (float): æ‰©å±•æ¯”ä¾‹ï¼Œé»˜è®¤0.5
            use_scc (bool): æ˜¯å¦åŒ…å«SCCæ³¨æ„åŠ›ï¼Œé»˜è®¤True

        ç½‘ç»œç»“æ„ï¼š
            è¾“å…¥
             â”‚
            â”Œâ”€â”´â”€â”€â”€â”€ æ®‹å·®è¿æ¥ï¼ˆå¯é€‰ï¼‰
            â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  â”‚ 1x1 Conv       â”‚ é€šé“è°ƒæ•´
            â”‚  â”‚ (c1 -> c_hidden)â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚         â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  â”‚ 3x3 Conv       â”‚ ç©ºé—´ç‰¹å¾æå–
            â”‚  â”‚ (c_hidden -> c_hidden)â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚         â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  â”‚ SCC Attention  â”‚ æ³¨æ„åŠ›å¢å¼ºï¼ˆå¯é€‰ï¼‰
            â”‚  â”‚ (c_hidden)     â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚         â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  â”‚ 1x1 Conv       â”‚ é€šé“æ¢å¤
            â”‚  â”‚ (c_hidden -> c2)â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚         â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                   è¾“å‡º (c2)
        """
        super().__init__()

        # è®¡ç®—éšè—å±‚é€šé“æ•°
        c_ = int(c2 * e)  # éšè—å±‚é€šé“æ•° = è¾“å‡ºé€šé“æ•° * æ‰©å±•æ¯”ä¾‹

        # ç¬¬ä¸€ä¸ª1x1å·ç§¯ï¼šé€šé“æ•°è°ƒæ•´å’Œé™ç»´
        self.cv1 = nn.Sequential(
            nn.Conv2d(c1, c_, 1, 1, bias=False),    # 1x1å·ç§¯è°ƒæ•´é€šé“
            nn.BatchNorm2d(c_),                      # æ‰¹å½’ä¸€åŒ–
            nn.ReLU6(inplace=True)                   # ReLU6æ¿€æ´»ï¼ˆç§»åŠ¨ç«¯å‹å¥½ï¼‰
        )

        # 3x3å·ç§¯ï¼šç©ºé—´ç‰¹å¾æå–
        self.cv2 = nn.Sequential(
            nn.Conv2d(c_, c_, 3, 1, 1, groups=g, bias=False),  # 3x3æ·±åº¦å¯åˆ†ç¦»å·ç§¯
            nn.BatchNorm2d(c_),                      # æ‰¹å½’ä¸€åŒ–
            nn.ReLU6(inplace=True)                   # ReLU6æ¿€æ´»
        )

        # æœ€åä¸€ä¸ª1x1å·ç§¯ï¼šé€šé“æ•°æ¢å¤åˆ°è¾“å‡ºç»´åº¦
        self.cv3 = nn.Sequential(
            nn.Conv2d(c_, c2, 1, 1, bias=False),    # 1x1å·ç§¯æ¢å¤é€šé“æ•°
            nn.BatchNorm2d(c2)                       # æ‰¹å½’ä¸€åŒ–
        )

        # SCCæ³¨æ„åŠ›æ¨¡å—ï¼ˆåœ¨æœ€ç»ˆæ¿€æ´»ä¹‹å‰åº”ç”¨ï¼‰
        self.use_scc = use_scc
        if use_scc:
            self.scc = SCC_Attention(c2)  # åœ¨è¾“å‡ºé€šé“ä¸Šåº”ç”¨æ³¨æ„åŠ›

        # æ®‹å·®è¿æ¥è®¾ç½®
        self.add = shortcut and c1 == c2  # åªæœ‰è¾“å…¥è¾“å‡ºé€šé“ç›¸åŒæ—¶æ‰ä½¿ç”¨æ®‹å·®è¿æ¥

        # æœ€ç»ˆæ¿€æ´»å‡½æ•°
        self.act = nn.ReLU6(inplace=True)

    def forward(self, x):
        """
        å¸¦å¯é€‰SCCæ³¨æ„åŠ›çš„å‰å‘ä¼ æ’­

        å‚æ•°ï¼š
            x (torch.Tensor): è¾“å…¥ç‰¹å¾å›¾ï¼Œå½¢çŠ¶ä¸º[B, C1, H, W]

        è¿”å›å€¼ï¼š
            torch.Tensor: è¾“å‡ºç‰¹å¾å›¾ï¼Œå½¢çŠ¶ä¸º[B, C2, H, W]

        å¤„ç†æµç¨‹ï¼š
            1. ä¾æ¬¡é€šè¿‡ä¸‰ä¸ªå·ç§¯å±‚è¿›è¡Œç‰¹å¾å˜æ¢
            2. å¯é€‰åº”ç”¨SCCæ³¨æ„åŠ›è¿›è¡Œç‰¹å¾å¢å¼º
            3. åº”ç”¨æœ€ç»ˆæ¿€æ´»å‡½æ•°
            4. å¯é€‰çš„æ®‹å·®è¿æ¥èåˆ

        æŠ€æœ¯ç‰¹ç‚¹ï¼š
            - æ ‡å‡†ç“¶é¢ˆæ¶æ„ï¼š1x1 -> 3x3 -> 1x1çš„é€šé“å˜åŒ–
            - æ³¨æ„åŠ›é›†æˆï¼šåœ¨ç‰¹å¾è¾“å‡ºå‰è¿›è¡Œæ³¨æ„åŠ›é‡æ ‡å®š
            - æ®‹å·®è¿æ¥ï¼šä¿æŒæ¢¯åº¦æµåŠ¨å’Œç‰¹å¾é‡ç”¨
            - ç§»åŠ¨ç«¯ä¼˜åŒ–ï¼šä½¿ç”¨ReLU6å’Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯
        """
        # æ ‡å‡†ç“¶é¢ˆå¤„ç†æµç¨‹
        y = self.cv3(self.cv2(self.cv1(x)))

        # å¯é€‰çš„SCCæ³¨æ„åŠ›å¢å¼º
        if self.use_scc:
            y = self.scc(y)  # åº”ç”¨ç©ºé—´-é€šé“äº¤å‰æ³¨æ„åŠ›

        # æœ€ç»ˆæ¿€æ´»
        y = self.act(y)

        # æ®‹å·®è¿æ¥ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
        return x + y if self.add else y


# æµ‹è¯•SCCå®ç°
if __name__ == "__main__":
    """
    SCCæ³¨æ„åŠ›æ¨¡å—çš„å®Œæ•´æµ‹è¯•éªŒè¯

    æµ‹è¯•å†…å®¹ï¼š
    1. ChannelAttentionï¼šé€šé“æ³¨æ„åŠ›æœºåˆ¶éªŒè¯
    2. SpatialAttentionï¼šç©ºé—´æ³¨æ„åŠ›æœºåˆ¶éªŒè¯
    3. SCC_Attentionï¼šç»„åˆæ³¨æ„åŠ›æœºåˆ¶éªŒè¯
    4. SCC_Bottleneckï¼šæ³¨æ„åŠ›å¢å¼ºç“¶é¢ˆå—éªŒè¯
    5. æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œå†…å­˜ä½¿ç”¨åˆ†æ

    æµ‹è¯•ç›®æ ‡ï¼š
        - éªŒè¯å„æ³¨æ„åŠ›ç»„ä»¶çš„æ­£ç¡®å®ç°
        - ç¡®ä¿SCCæœºåˆ¶çš„æœ‰æ•ˆé›†æˆ
        - è¯„ä¼°è®¡ç®—æ•ˆç‡å’Œæ€§èƒ½æå‡
        - ä¸ºå®é™…åº”ç”¨æä¾›ä½¿ç”¨æŒ‡å¯¼

    æŠ€æœ¯éªŒè¯ï¼š
        - æ³¨æ„åŠ›æƒé‡ç”Ÿæˆçš„æ­£ç¡®æ€§
        - ç‰¹å¾å¢å¼ºæ•ˆæœçš„é‡åŒ–è¯„ä¼°
        - è®¡ç®—å¤æ‚åº¦å’Œå†…å­˜ä½¿ç”¨çš„åˆ†æ
        - ä¸ç°æœ‰æ¶æ„çš„å…¼å®¹æ€§æµ‹è¯•

    è¾“å‡ºä¿¡æ¯ï¼š
        - å„ç»„ä»¶çš„éªŒè¯çŠ¶æ€
        - æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ
        - èµ„æºä½¿ç”¨ç»Ÿè®¡
        - ä½¿ç”¨å»ºè®®å’Œæœ€ä½³å®è·µ
    """
    from utils import setup_device, validate_model, benchmark_inference

    # åˆå§‹åŒ–è®¡ç®—è®¾å¤‡
    device = setup_device()

    # æµ‹è¯•å„ä¸ªç»„ä»¶
    print("\nğŸ”§ æµ‹è¯•SCCç»„ä»¶:")

    # é€šé“æ³¨æ„åŠ›æµ‹è¯•
    ca = ChannelAttention(channels=256)
    print("é€šé“æ³¨æ„åŠ›:")
    validate_model(ca, device, input_size=(1, 256, 32, 32))

    # ç©ºé—´æ³¨æ„åŠ›æµ‹è¯•
    sa = SpatialAttention(kernel_size=7)
    print("ç©ºé—´æ³¨æ„åŠ›:")
    validate_model(sa, device, input_size=(1, 256, 32, 32))

    # SCCæ³¨æ„åŠ›æµ‹è¯•
    scc = SCC_Attention(channels=256)
    print("SCCæ³¨æ„åŠ›:")
    validate_model(scc, device, input_size=(1, 256, 32, 32))

    # SCCç“¶é¢ˆå—æµ‹è¯•
    scc_bottleneck = SCC_Bottleneck(c1=256, c2=256, use_scc=True)
    print("SCCç“¶é¢ˆå—:")
    validate_model(scc_bottleneck, device, input_size=(1, 256, 32, 32))

    # æ€§èƒ½åŸºå‡†æµ‹è¯•
    print("\nğŸ“Š Benchmarking SCC Components:")
    benchmark_inference(scc, device, input_size=(1, 256, 32, 32))
    benchmark_inference(scc_bottleneck, device, input_size=(1, 256, 32, 32))

    print("\nâœ… SCC Attention modules tested successfully!")
